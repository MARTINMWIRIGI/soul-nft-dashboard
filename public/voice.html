<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Cultural Vault — Voice Recording Layer</title>
  <!-- Tailwind CDN for quick styling (drop-in for your project) -->
  <script src="https://cdn.tailwindcss.com"></script>
  <style>
    /* Small custom styles for waveform canvas & capsule card */
    .wave-canvas { width: 100%; height: 120px; background: linear-gradient(180deg,#071025 0%, #02111b 100%); border-radius: .5rem; }
    .breathing { animation: breathe 2.8s infinite ease-in-out; }
    @keyframes breathe { 0%{ transform: scale(1);} 50%{ transform: scale(1.01);} 100%{ transform: scale(1);} }
    .capsule { background: linear-gradient(90deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01)); border: 1px solid rgba(255,255,255,0.04); }
  </style>
</head>
<body class="bg-[#041025] text-slate-100 antialiased p-6 min-h-screen">
  <main class="max-w-4xl mx-auto">
    <!-- Header / Hero -->
    <section class="mb-6">
      <div class="flex items-start gap-4">
        <div class="p-4 rounded-2xl capsule shadow-lg w-full">
          <h1 class="text-3xl font-semibold">Record Your Eternal Voice</h1>
          <p class="mt-2 text-slate-300">Voices are rivers of memory. Speak — and seal a life-capsule on-chain.</p>
          <div class="mt-4 flex gap-3 flex-wrap">
            <button id="startBtn" class="bg-emerald-500 hover:bg-emerald-600 text-black px-4 py-2 rounded-full font-medium">Start Recording</button>
            <button id="stopBtn" class="bg-red-600 hover:bg-red-700 text-white px-4 py-2 rounded-full font-medium" disabled>Stop</button>
            <button id="reRecordBtn" class="bg-slate-700 px-4 py-2 rounded-full" disabled>Re-record</button>
            <button id="previewBtn" class="bg-sky-600 px-4 py-2 rounded-full" disabled>Preview</button>
            <button id="mintBtn" class="bg-violet-600 px-4 py-2 rounded-full ml-auto" disabled>Mint Voice NFT</button>
          </div>
        </div>
      </div>
    </section>

    <!-- Recording area with waveform -->
    <section class="mb-6">
      <div class="capsule p-4 rounded-2xl">
        <div class="wave-canvas relative overflow-hidden p-3 breathing" id="waveWrap">
          <canvas id="waveCanvas" width="1200" height="120" class="w-full h-full"></canvas>
          <div id="liveLabel" class="absolute left-3 top-3 text-xs text-slate-300/80">Idle • No input</div>
          <div id="timer" class="absolute right-3 top-3 text-xs text-slate-300/80">00:00</div>
        </div>

        <div class="mt-3 grid grid-cols-1 md:grid-cols-3 gap-3 text-sm">
          <div>
            <label class="block text-slate-300 text-xs mb-1">Prompt</label>
            <select id="promptSelect" class="w-full rounded p-2 text-black">
              <option value="poetic">Poetic / Timeless</option>
              <option value="minimal">Minimal / Oath-like</option>
              <option value="futuristic">Futuristic / AI-flavored</option>
            </select>
          </div>
          <div class="md:col-span-2">
            <label class="block text-slate-300 text-xs mb-1">Suggested text (editable)</label>
            <textarea id="promptText" rows="3" class="w-full rounded p-2 text-black" spellcheck="false"></textarea>
          </div>
        </div>

        <div class="mt-3 text-slate-400 text-sm">Tips: hold the device steady, record in a quiet place, and speak slowly — preserve dialects and unique intonations.</div>
      </div>
    </section>

    <!-- Preview + Metadata -->
    <section class="mb-6">
      <div class="capsule p-4 rounded-2xl">
        <h2 class="text-lg font-medium">Preview & Metadata</h2>
        <div class="mt-3 flex items-center gap-4">
          <audio id="audioPreview" controls class="w-full" hidden></audio>
          <div class="w-40">
            <label class="block text-slate-300 text-xs mb-1">Location</label>
            <input id="locationInput" type="text" placeholder="Kenya, Nairobi" class="w-full rounded p-2 text-black" />
          </div>
          <div class="w-40">
            <label class="block text-slate-300 text-xs mb-1">Date</label>
            <input id="dateInput" type="date" class="w-full rounded p-2 text-black" />
          </div>
          <div class="w-40">
            <label class="block text-slate-300 text-xs mb-1">Language / Dialect</label>
            <input id="langInput" type="text" placeholder="Swahili / Kikuyu" class="w-full rounded p-2 text-black" />
          </div>
        </div>

        <div class="mt-4 text-slate-400 text-sm">After previewing you can mint. Minting will package the audio + generated waveform-art + metadata into a JSON tokenURI suitable for IPFS and your NFT contract.</div>
      </div>
    </section>

    <!-- Gallery of minted capsules (demo) -->
    <section>
      <h3 class="text-lg mb-2">Minted Voice Capsules</h3>
      <div id="capsules" class="grid gap-3 grid-cols-1 md:grid-cols-2"></div>
    </section>

    <p class="mt-8 text-slate-500 text-sm">Cultural Vault — Voice Recording Layer • Make sure to wire the <code>mintVoiceNFT()</code> function to your smart contract/minting backend.</p>
  </main>

  <!-- Application Script -->
  <script>
  /*
    Voice Recording Layer JS
    Features:
      - Live waveform using WebAudio API
      - MediaRecorder capture -> WAV/Blob preview
      - Generate a simple SVG waveform image as 'art' for the NFT
      - Create metadata JSON (tokenURI) ready for IPFS upload
      - Placeholder mintVoiceNFT() to be connected to your backend or wallet
  */

  // Suggested prompt templates
  const prompts = {
    poetic: `The year is 2025, September.\nI leave this voice as proof that I lived among humans.\nOn a continent called Africa, in a land called Kenya, I walked, I breathed, I dreamed.\nThis is my voice, my eternal witness.\nNow preserved on the blockchain, a life-capsule that will never fade.`,
    minimal: `2025. September.\nThis is my voice. My truth. My oath.\nI lived. I existed. I sign.\nForever recorded on the blockchain.`,
    futuristic: `Year: 2025, Cycle of September.\nThis is my human voice — encrypted in time, anchored in Africa, Kenya.\nI declare my existence in the age of memory-tech.\nThe blockchain now carries my life-capsule, eternal and unbroken.\nTo the future: I was here.`
  };

  // DOM refs
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const reRecordBtn = document.getElementById('reRecordBtn');
  const previewBtn = document.getElementById('previewBtn');
  const mintBtn = document.getElementById('mintBtn');
  const promptSelect = document.getElementById('promptSelect');
  const promptText = document.getElementById('promptText');
  const waveCanvas = document.getElementById('waveCanvas');
  const waveWrap = document.getElementById('waveWrap');
  const liveLabel = document.getElementById('liveLabel');
  const timerEl = document.getElementById('timer');
  const audioPreview = document.getElementById('audioPreview');
  const capsules = document.getElementById('capsules');
  const dateInput = document.getElementById('dateInput');
  const locationInput = document.getElementById('locationInput');
  const langInput = document.getElementById('langInput');

  // state
  let audioContext, analyser, dataArray, sourceNode;
  let mediaRecorder; let recordedChunks = [];
  let recordStart = 0; let timerInterval;
  let mediaStream;

  // init
  promptText.value = prompts[ promptSelect.value ];
  dateInput.value = new Date().toISOString().slice(0,10);

  promptSelect.addEventListener('change', ()=> {
    promptText.value = prompts[ promptSelect.value ];
  });

  // wave drawing util
  function drawWave() {
    if (!analyser) return;
    requestAnimationFrame(drawWave);
    analyser.getByteTimeDomainData(dataArray);
    const ctx = waveCanvas.getContext('2d');
    const w = waveCanvas.width; const h = waveCanvas.height;
    ctx.clearRect(0,0,w,h);
    // background gradient
    const grad = ctx.createLinearGradient(0,0,0,h);
    grad.addColorStop(0,'rgba(255,255,255,0.03)');
    grad.addColorStop(1,'rgba(255,255,255,0.01)');
    ctx.fillStyle = grad; ctx.fillRect(0,0,w,h);

    ctx.lineWidth = 2;
    ctx.strokeStyle = 'rgba(139,92,246,0.9)';
    ctx.beginPath();
    const sliceWidth = w * 1.0 / dataArray.length;
    let x = 0;
    for (let i=0;i<dataArray.length;i++){
      const v = dataArray[i] / 128.0;
      const y = v * h/2;
      if(i===0) ctx.moveTo(x,y); else ctx.lineTo(x,y);
      x += sliceWidth;
    }
    ctx.stroke();
  }

  // start capturing audio
  startBtn.addEventListener('click', async ()=>{
    try{
      mediaStream = await navigator.mediaDevices.getUserMedia({audio:true});
      audioContext = new (window.AudioContext || window.webkitAudioContext)();
      sourceNode = audioContext.createMediaStreamSource(mediaStream);
      analyser = audioContext.createAnalyser(); analyser.fftSize = 2048;
      dataArray = new Uint8Array(analyser.fftSize);
      sourceNode.connect(analyser);
      drawWave();

      // MediaRecorder to capture data
      recordedChunks = [];
      mediaRecorder = new MediaRecorder(mediaStream, {mimeType:'audio/webm'});
      mediaRecorder.ondataavailable = e => { if(e.data && e.data.size) recordedChunks.push(e.data); };
      mediaRecorder.onstop = onRecordingStop;
      mediaRecorder.start();

      startTimer();
      liveLabel.textContent = 'Recording...';
      startBtn.disabled = true; stopBtn.disabled = false; reRecordBtn.disabled = true; previewBtn.disabled=true; mintBtn.disabled=true;
    } catch(err){
      console.error(err); alert('Could not access microphone. Check permissions.');
    }
  });

  // stop capturing
  stopBtn.addEventListener('click', ()=>{
    if(mediaRecorder && mediaRecorder.state === 'recording') mediaRecorder.stop();
    if(mediaStream){ mediaStream.getTracks().forEach(t=>t.stop()); }
    stopTimer();
    liveLabel.textContent = 'Processing...';
    startBtn.disabled = false; stopBtn.disabled = true; reRecordBtn.disabled = false;
  });

  function startTimer(){ recordStart = Date.now(); timerInterval = setInterval(()=>{
    const s = Math.floor((Date.now() - recordStart)/1000);
    const mm = String(Math.floor(s/60)).padStart(2,'0');
    const ss = String(s%60).padStart(2,'0');
    timerEl.textContent = mm+':'+ss;
  },250);} 
  function stopTimer(){ clearInterval(timerInterval); }

  // when recording stops
  function onRecordingStop(){
    const blob = new Blob(recordedChunks, {type:'audio/webm'});
    const url = URL.createObjectURL(blob);
    audioPreview.src = url; audioPreview.hidden = false; previewBtn.disabled = false; mintBtn.disabled = false; liveLabel.textContent = 'Ready to preview';
    // keep blob for minting
    audioPreview.dataset.blobUrl = url; audioPreview.dataset.blobType = blob.type;
    audioPreview._blob = blob;
  }

  // re-record
  reRecordBtn.addEventListener('click', ()=>{
    // clear existing
    audioPreview.hidden = true; audioPreview.src = '';
    recordedChunks = []; audioPreview._blob = null;
    promptText.value = prompts[ promptSelect.value ];
    liveLabel.textContent = 'Idle • No input'; timerEl.textContent = '00:00';
    reRecordBtn.disabled = true; previewBtn.disabled = true; mintBtn.disabled = true;
  });

  // preview binds to the audio element control – nothing special here
  previewBtn.addEventListener('click', ()=>{
    audioPreview.play();
  });

  // generate simple SVG waveform art from audio buffer amplitude peaks
  async function generateWaveformSVG(blob){
    // decode via AudioContext
    const arrayBuffer = await blob.arrayBuffer();
    const ctx = new (window.OfflineAudioContext || window.AudioContext)(1,44100*3,44100);
    const audioBuffer = await ctx.decodeAudioData(arrayBuffer);
    const channelData = audioBuffer.getChannelData(0);
    const samples = 200; // reduce for svg
    const block = Math.floor(channelData.length / samples);
    const points = [];
    for(let i=0;i<samples;i++){
      let sum=0; for(let j=0;j<block;j++) sum += Math.abs(channelData[i*block + j] || 0);
      points.push(sum / block);
    }
    const max = Math.max(...points);
    const w = 1200, h = 300;
    const path = points.map((p,i)=>{ const x = Math.round(i*(w/samples)); const y = Math.round((1 - (p/max))*(h-20))+10; return `${i===0? 'M':'L'} ${x} ${y}`; }).join(' ');
    const svg = `<svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 ${w} ${h}' width='${w}' height='${h}'><defs><linearGradient id='g' x1='0' x2='1'><stop offset='0' stop-color='#7c3aed'/><stop offset='1' stop-color='#06b6d4'/></linearGradient></defs><rect width='100%' height='100%' fill='transparent'/><path d='${path}' fill='none' stroke='url(#g)' stroke-width='14' stroke-linecap='round' stroke-linejoin='round' opacity='0.95'/></svg>`;
    return new Blob([svg], {type:'image/svg+xml'});
  }

  // mint flow (placeholder) — creates token metadata and returns a JSON blob ready to upload
  mintBtn.addEventListener('click', async ()=>{
    if(!audioPreview._blob){ alert('No recording found.'); return; }
    mintBtn.disabled = true; mintBtn.textContent = 'Preparing...';

    const audioBlob = audioPreview._blob;
    const waveformBlob = await generateWaveformSVG(audioBlob);

    // create metadata JSON
    const metadata = {
      name: `Voice Capsule — ${new Date().toISOString()}`,
      description: promptText.value.substring(0,200),
      attributes: [
        {trait_type:'location', value: locationInput.value || 'Kenya'},
        {trait_type:'date', value: dateInput.value || (new Date().toISOString().slice(0,10))},
        {trait_type:'language', value: langInput.value || 'unknown'}
      ],
      // NOTE: these are placeholders. Replace with IPFS URLs after upload
      audio_file: 'UPLOAD_AUDIO_AND_PASTE_IPFS_URL_HERE',
      image_file: 'UPLOAD_SVG_AND_PASTE_IPFS_URL_HERE'
    };

    // The recommended production flow:
    // 1) Upload audioBlob and waveformBlob to IPFS (or your storage) — get back ipfs:// or https://gateway URLs
    // 2) Replace the tokenURI fields above with those URLs
    // 3) Call your smart contract mint function (via web3/ethers or backend) passing the tokenURI

    // For demo: pack into a local card and offer downloads
    const card = document.createElement('div'); card.className = 'p-4 capsule rounded-lg';
    card.innerHTML = `<div class=\"flex items-start gap-3\"><div class=\"w-20 h-20 bg-slate-800 rounded-md flex items-center justify-center\">SVG</div><div class=\"flex-1\"><strong>${metadata.name}</strong><div class=\"text-xs text-slate-400 mt-1\">${metadata.description}</div><div class=\"mt-2 flex gap-2\"><a class=\"text-xs underline\" download=\"voice.webm\">Download Audio</a><a class=\"text-xs underline\">Download Waveform SVG</a></div></div></div>`;

    // attach blob URLs to the links
    const audioUrl = URL.createObjectURL(audioBlob);
    const svgUrl = URL.createObjectURL(waveformBlob);
    card.querySelector('a[download]').href = audioUrl;
    card.querySelectorAll('a')[1].href = svgUrl; card.querySelectorAll('a')[1].setAttribute('download','waveform.svg');

    capsules.prepend(card);

    // reset UI states
    mintBtn.textContent = 'Mint Voice NFT'; mintBtn.disabled = false; liveLabel.textContent = 'Mint ready (demo)';

    // Developer note: here you would upload blobs and call your contract. Example pseudo:
    /*
      const audioIpfs = await uploadToIpfs(audioBlob);
      const imageIpfs = await uploadToIpfs(waveformBlob);
      metadata.audio_file = audioIpfs;
      metadata.image_file = imageIpfs;
      const metadataBlob = new Blob([JSON.stringify(metadata)], {type:'application/json'});
      const metadataIpfs = await uploadToIpfs(metadataBlob);
      // then call your contract: await contract.mintTo(userAddress, metadataIpfs);
    */

  });

  // OPTIONAL: helper to detect permission / availability
  if(!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia){
    startBtn.disabled = true; liveLabel.textContent = 'Microphone not supported in this browser.';
  }

  </script>
</body>
</html>
